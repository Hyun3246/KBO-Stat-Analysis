{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4V4Qm6SnHb9T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/sample_data/KBO_24_STAT_edited.csv')"
      ],
      "metadata": {
        "id": "ksWFQnpBH9id"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHA9yeQfH--N",
        "outputId": "50926fad-cdca-43fd-9a8b-968068d6e5bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50 entries, 0 to 49\n",
            "Data columns (total 14 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   WAR     50 non-null     float64\n",
            " 1   AB      50 non-null     int64  \n",
            " 2   R       50 non-null     int64  \n",
            " 3   HR      50 non-null     int64  \n",
            " 4   TB      50 non-null     int64  \n",
            " 5   RBI     50 non-null     int64  \n",
            " 6   BB      50 non-null     int64  \n",
            " 7   IB      50 non-null     int64  \n",
            " 8   SF      50 non-null     int64  \n",
            " 9   AVG     50 non-null     float64\n",
            " 10  OBP     50 non-null     float64\n",
            " 11  SLG     50 non-null     float64\n",
            " 12  OPS     50 non-null     float64\n",
            " 13  wRC+    50 non-null     float64\n",
            "dtypes: float64(6), int64(8)\n",
            "memory usage: 5.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 타입 변경\n",
        "df = df.astype('float32')"
      ],
      "metadata": {
        "id": "o9Hi_Ti2IE6Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLvEr8x7JmaY",
        "outputId": "114f1527-efa9-4525-bc93-fefd0fff709f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50 entries, 0 to 49\n",
            "Data columns (total 14 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   WAR     50 non-null     float32\n",
            " 1   AB      50 non-null     float32\n",
            " 2   R       50 non-null     float32\n",
            " 3   HR      50 non-null     float32\n",
            " 4   TB      50 non-null     float32\n",
            " 5   RBI     50 non-null     float32\n",
            " 6   BB      50 non-null     float32\n",
            " 7   IB      50 non-null     float32\n",
            " 8   SF      50 non-null     float32\n",
            " 9   AVG     50 non-null     float32\n",
            " 10  OBP     50 non-null     float32\n",
            " 11  SLG     50 non-null     float32\n",
            " 12  OPS     50 non-null     float32\n",
            " 13  wRC+    50 non-null     float32\n",
            "dtypes: float32(14)\n",
            "memory usage: 2.9 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train set과 test set 분리\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_set.to_csv(\"KB0_24_train.csv\", index = False)\n",
        "test_set.to_csv(\"KB0_24_test.csv\", index = False)"
      ],
      "metadata": {
        "id": "0IK0RNrjJn1Y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 변수와 타깃값 분리\n",
        "X = train_set.drop(\"WAR\", axis=1)\n",
        "y = train_set[\"WAR\"].copy()"
      ],
      "metadata": {
        "id": "FSzqTjfjJ0WS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_set.drop(\"WAR\", axis=1).to_numpy()\n",
        "y = train_set[\"WAR\"].copy().to_numpy()\n",
        "X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCVuNY56KPSh",
        "outputId": "e4ec7e68-527e-4201-a548-dd40f504929a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[4.530e+02, 1.020e+02, 3.000e+00, 1.740e+02, 3.600e+01, 6.000e+01,\n",
              "         2.000e+00, 1.000e+00, 3.160e-01, 4.050e-01, 3.840e-01, 7.890e-01,\n",
              "         1.127e+02],\n",
              "        [5.090e+02, 9.000e+01, 1.100e+01, 2.330e+02, 7.500e+01, 4.700e+01,\n",
              "         2.000e+00, 7.000e+00, 3.260e-01, 3.830e-01, 4.580e-01, 8.410e-01,\n",
              "         1.241e+02],\n",
              "        [5.330e+02, 8.300e+01, 3.400e+01, 2.600e+02, 1.070e+02, 4.900e+01,\n",
              "         1.000e+00, 3.000e+00, 2.460e-01, 3.160e-01, 4.880e-01, 8.040e-01,\n",
              "         1.045e+02],\n",
              "        [4.680e+02, 9.300e+01, 3.700e+01, 2.780e+02, 1.070e+02, 5.500e+01,\n",
              "         1.000e+00, 7.000e+00, 2.910e-01, 3.840e-01, 5.940e-01, 9.780e-01,\n",
              "         1.459e+02],\n",
              "        [4.930e+02, 9.200e+01, 3.300e+01, 3.090e+02, 1.150e+02, 5.500e+01,\n",
              "         4.000e+00, 6.000e+00, 3.430e-01, 4.170e-01, 6.270e-01, 1.044e+00,\n",
              "         1.623e+02],\n",
              "        [5.240e+02, 9.600e+01, 5.000e+00, 2.150e+02, 7.300e+01, 9.600e+01,\n",
              "         3.000e+00, 4.000e+00, 3.360e-01, 4.470e-01, 4.100e-01, 8.570e-01,\n",
              "         1.423e+02],\n",
              "        [4.730e+02, 8.200e+01, 1.300e+01, 1.900e+02, 6.000e+01, 4.900e+01,\n",
              "         1.000e+00, 1.000e+00, 2.660e-01, 3.520e-01, 4.020e-01, 7.540e-01,\n",
              "         1.036e+02],\n",
              "        [4.730e+02, 6.400e+01, 1.300e+01, 2.020e+02, 6.600e+01, 4.900e+01,\n",
              "         1.000e+00, 4.000e+00, 3.000e-01, 3.700e-01, 4.270e-01, 7.970e-01,\n",
              "         1.079e+02],\n",
              "        [4.360e+02, 6.100e+01, 2.000e+01, 2.020e+02, 8.300e+01, 4.900e+01,\n",
              "         1.000e+00, 5.000e+00, 2.710e-01, 3.510e-01, 4.630e-01, 8.140e-01,\n",
              "         1.089e+02],\n",
              "        [3.850e+02, 6.100e+01, 9.000e+00, 1.460e+02, 4.900e+01, 5.100e+01,\n",
              "         1.000e+00, 5.000e+00, 2.520e-01, 3.710e-01, 3.790e-01, 7.500e-01,\n",
              "         1.084e+02],\n",
              "        [5.410e+02, 8.200e+01, 2.100e+01, 2.910e+02, 1.180e+02, 2.800e+01,\n",
              "         3.000e+00, 9.000e+00, 3.600e-01, 3.990e-01, 5.380e-01, 9.370e-01,\n",
              "         1.372e+02],\n",
              "        [5.210e+02, 8.100e+01, 1.800e+01, 2.480e+02, 8.100e+01, 3.200e+01,\n",
              "         4.000e+00, 4.000e+00, 2.800e-01, 3.280e-01, 4.760e-01, 8.040e-01,\n",
              "         1.043e+02],\n",
              "        [5.320e+02, 9.700e+01, 1.400e+01, 2.410e+02, 8.500e+01, 6.700e+01,\n",
              "         1.000e+00, 5.000e+00, 2.930e-01, 3.760e-01, 4.530e-01, 8.290e-01,\n",
              "         1.130e+02],\n",
              "        [4.880e+02, 7.800e+01, 1.600e+01, 2.000e+02, 7.300e+01, 5.200e+01,\n",
              "         1.000e+00, 8.000e+00, 2.580e-01, 3.370e-01, 4.100e-01, 7.470e-01,\n",
              "         9.500e+01],\n",
              "        [5.260e+02, 8.800e+01, 2.400e+01, 2.390e+02, 8.900e+01, 6.000e+01,\n",
              "         9.000e+00, 4.000e+00, 2.720e-01, 3.560e-01, 4.540e-01, 8.100e-01,\n",
              "         1.065e+02],\n",
              "        [5.440e+02, 1.430e+02, 3.800e+01, 3.520e+02, 1.090e+02, 6.600e+01,\n",
              "         7.000e+00, 7.000e+00, 3.470e-01, 4.200e-01, 6.470e-01, 1.067e+00,\n",
              "         1.725e+02],\n",
              "        [4.250e+02, 6.700e+01, 2.200e+01, 2.120e+02, 1.090e+02, 5.200e+01,\n",
              "         4.000e+00, 5.000e+00, 2.800e-01, 3.610e-01, 4.990e-01, 8.600e-01,\n",
              "         1.190e+02],\n",
              "        [3.890e+02, 7.100e+01, 1.400e+01, 1.630e+02, 6.600e+01, 5.800e+01,\n",
              "         1.000e+00, 3.000e+00, 2.600e-01, 3.650e-01, 4.190e-01, 7.840e-01,\n",
              "         1.048e+02],\n",
              "        [4.560e+02, 6.500e+01, 2.800e+01, 2.210e+02, 7.900e+01, 4.500e+01,\n",
              "         0.000e+00, 4.000e+00, 2.520e-01, 3.210e-01, 4.850e-01, 8.060e-01,\n",
              "         9.780e+01],\n",
              "        [5.270e+02, 9.900e+01, 3.200e+01, 3.020e+02, 1.320e+02, 6.100e+01,\n",
              "         6.000e+00, 1.300e+01, 3.190e-01, 3.840e-01, 5.730e-01, 9.570e-01,\n",
              "         1.478e+02],\n",
              "        [5.150e+02, 8.600e+01, 5.000e+00, 1.990e+02, 6.100e+01, 4.800e+01,\n",
              "         1.000e+00, 6.000e+00, 3.070e-01, 3.630e-01, 3.860e-01, 7.490e-01,\n",
              "         9.570e+01],\n",
              "        [5.190e+02, 8.000e+01, 2.200e+01, 2.630e+02, 1.010e+02, 6.500e+01,\n",
              "         3.000e+00, 1.300e+01, 3.010e-01, 3.720e-01, 5.070e-01, 8.790e-01,\n",
              "         1.304e+02],\n",
              "        [4.380e+02, 7.500e+01, 9.000e+00, 1.840e+02, 5.600e+01, 5.000e+01,\n",
              "         0.000e+00, 5.000e+00, 2.920e-01, 3.710e-01, 4.200e-01, 7.910e-01,\n",
              "         1.101e+02],\n",
              "        [5.720e+02, 1.080e+02, 3.200e+01, 3.250e+02, 1.120e+02, 8.800e+01,\n",
              "         9.000e+00, 4.000e+00, 3.290e-01, 4.210e-01, 5.680e-01, 9.890e-01,\n",
              "         1.533e+02],\n",
              "        [4.180e+02, 5.300e+01, 1.900e+01, 1.880e+02, 8.100e+01, 6.000e+01,\n",
              "         1.000e+00, 8.000e+00, 2.680e-01, 3.550e-01, 4.500e-01, 8.050e-01,\n",
              "         1.042e+02],\n",
              "        [5.270e+02, 8.800e+01, 1.900e+01, 2.730e+02, 1.040e+02, 6.400e+01,\n",
              "         2.000e+00, 8.000e+00, 3.400e-01, 4.090e-01, 5.180e-01, 9.270e-01,\n",
              "         1.489e+02],\n",
              "        [4.830e+02, 8.900e+01, 1.100e+01, 2.020e+02, 4.900e+01, 5.000e+01,\n",
              "         0.000e+00, 2.000e+00, 2.750e-01, 3.450e-01, 4.180e-01, 7.630e-01,\n",
              "         9.220e+01],\n",
              "        [4.070e+02, 5.900e+01, 7.000e+00, 1.910e+02, 6.600e+01, 6.900e+01,\n",
              "         1.000e+00, 7.000e+00, 3.120e-01, 4.110e-01, 4.690e-01, 8.800e-01,\n",
              "         1.297e+02],\n",
              "        [4.170e+02, 6.900e+01, 7.000e+00, 1.780e+02, 6.100e+01, 3.600e+01,\n",
              "         0.000e+00, 6.000e+00, 3.090e-01, 3.840e-01, 4.270e-01, 8.110e-01,\n",
              "         1.220e+02],\n",
              "        [4.820e+02, 7.200e+01, 6.000e+00, 1.730e+02, 5.600e+01, 4.600e+01,\n",
              "         0.000e+00, 9.000e+00, 2.630e-01, 3.360e-01, 3.590e-01, 6.950e-01,\n",
              "         8.610e+01],\n",
              "        [4.340e+02, 5.800e+01, 2.000e+01, 2.000e+02, 8.000e+01, 5.500e+01,\n",
              "         2.000e+00, 7.000e+00, 2.720e-01, 3.490e-01, 4.610e-01, 8.100e-01,\n",
              "         1.110e+02],\n",
              "        [4.300e+02, 5.700e+01, 1.700e+01, 2.060e+02, 9.400e+01, 4.000e+01,\n",
              "         5.000e+00, 6.000e+00, 3.140e-01, 3.790e-01, 4.790e-01, 8.580e-01,\n",
              "         1.253e+02],\n",
              "        [5.520e+02, 9.200e+01, 2.600e+01, 2.850e+02, 9.700e+01, 4.100e+01,\n",
              "         4.000e+00, 5.000e+00, 3.100e-01, 3.590e-01, 5.160e-01, 8.750e-01,\n",
              "         1.212e+02],\n",
              "        [4.050e+02, 5.700e+01, 7.000e+00, 1.480e+02, 5.300e+01, 4.700e+01,\n",
              "         3.000e+00, 3.000e+00, 2.620e-01, 3.510e-01, 3.650e-01, 7.160e-01,\n",
              "         8.640e+01],\n",
              "        [4.030e+02, 4.800e+01, 1.900e+01, 2.000e+02, 7.700e+01, 3.500e+01,\n",
              "         2.000e+00, 6.000e+00, 3.030e-01, 3.650e-01, 4.960e-01, 8.610e-01,\n",
              "         1.156e+02],\n",
              "        [4.570e+02, 7.500e+01, 8.000e+00, 2.040e+02, 5.000e+01, 5.400e+01,\n",
              "         4.000e+00, 4.000e+00, 3.280e-01, 4.060e-01, 4.460e-01, 8.520e-01,\n",
              "         1.296e+02],\n",
              "        [4.230e+02, 5.700e+01, 1.700e+01, 2.050e+02, 8.200e+01, 4.900e+01,\n",
              "         0.000e+00, 5.000e+00, 2.930e-01, 3.690e-01, 4.850e-01, 8.540e-01,\n",
              "         1.157e+02],\n",
              "        [4.160e+02, 6.600e+01, 1.300e+01, 1.880e+02, 7.700e+01, 7.700e+01,\n",
              "         1.000e+00, 6.000e+00, 3.000e-01, 4.170e-01, 4.520e-01, 8.690e-01,\n",
              "         1.379e+02],\n",
              "        [5.100e+02, 9.500e+01, 4.000e+00, 1.840e+02, 4.700e+01, 7.100e+01,\n",
              "         1.000e+00, 8.000e+00, 2.840e-01, 3.760e-01, 3.610e-01, 7.370e-01,\n",
              "         1.002e+02],\n",
              "        [4.550e+02, 7.500e+01, 2.400e+01, 2.210e+02, 7.000e+01, 6.300e+01,\n",
              "         5.000e+00, 2.000e+00, 2.750e-01, 3.640e-01, 4.860e-01, 8.500e-01,\n",
              "         1.167e+02]], dtype=float32),\n",
              " array([3.98, 5.16, 2.17, 4.55, 5.69, 5.06, 1.96, 1.33, 1.14, 3.75, 4.42,\n",
              "        3.71, 3.13, 2.24, 2.41, 8.32, 1.54, 3.03, 2.29, 5.06, 2.78, 4.32,\n",
              "        2.17, 6.5 , 3.23, 6.13, 1.76, 2.22, 3.2 , 2.03, 4.4 , 3.21, 3.42,\n",
              "        1.01, 3.32, 4.7 , 1.8 , 3.77, 2.97, 2.13], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스케일링\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "X_scaled = std_scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "drdNrRWQRwf6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "W = tf.Variable(tf.random.normal([13, 1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')"
      ],
      "metadata": {
        "id": "-rzIixWwL517"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 비용함수 정의\n",
        "def loss_fn(model, features, labels):\n",
        "  hypothesis = model(features)\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - labels))\n",
        "  return cost"
      ],
      "metadata": {
        "id": "2q4LL5hKNBUT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "aCWuWQy-NNy8"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b"
      ],
      "metadata": {
        "id": "bUuki3RgNbYQ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10000\n",
        "\n",
        "for i in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = loss_fn(predict, X_scaled, y)\n",
        "\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f'{i} loss: {loss_fn(predict, X_scaled, y)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Q2SVV2NcMh",
        "outputId": "2085dedf-172f-4c12-be9e-b61f6efdcba9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss: 2.5097250938415527\n",
            "100 loss: 2.509671688079834\n",
            "200 loss: 2.5096187591552734\n",
            "300 loss: 2.509566307067871\n",
            "400 loss: 2.5095131397247314\n",
            "500 loss: 2.5094614028930664\n",
            "600 loss: 2.5094099044799805\n",
            "700 loss: 2.5093584060668945\n",
            "800 loss: 2.509307622909546\n",
            "900 loss: 2.5092570781707764\n",
            "1000 loss: 2.509206533432007\n",
            "1100 loss: 2.5091569423675537\n",
            "1200 loss: 2.5091073513031006\n",
            "1300 loss: 2.5090579986572266\n",
            "1400 loss: 2.5090091228485107\n",
            "1500 loss: 2.508960723876953\n",
            "1600 loss: 2.5089125633239746\n",
            "1700 loss: 2.508864402770996\n",
            "1800 loss: 2.508816957473755\n",
            "1900 loss: 2.5087697505950928\n",
            "2000 loss: 2.5087225437164307\n",
            "2100 loss: 2.508676052093506\n",
            "2200 loss: 2.50862979888916\n",
            "2300 loss: 2.5085837841033936\n",
            "2400 loss: 2.508538007736206\n",
            "2500 loss: 2.5084924697875977\n",
            "2600 loss: 2.5084471702575684\n",
            "2700 loss: 2.5084023475646973\n",
            "2800 loss: 2.5083577632904053\n",
            "2900 loss: 2.5083136558532715\n",
            "3000 loss: 2.5082695484161377\n",
            "3100 loss: 2.508225917816162\n",
            "3200 loss: 2.5081827640533447\n",
            "3300 loss: 2.5081393718719482\n",
            "3400 loss: 2.50809645652771\n",
            "3500 loss: 2.508053779602051\n",
            "3600 loss: 2.508011817932129\n",
            "3700 loss: 2.507969617843628\n",
            "3800 loss: 2.507927656173706\n",
            "3900 loss: 2.5078861713409424\n",
            "4000 loss: 2.507844924926758\n",
            "4100 loss: 2.5078041553497314\n",
            "4200 loss: 2.507763385772705\n",
            "4300 loss: 2.507722854614258\n",
            "4400 loss: 2.5076828002929688\n",
            "4500 loss: 2.5076427459716797\n",
            "4600 loss: 2.5076029300689697\n",
            "4700 loss: 2.507563352584839\n",
            "4800 loss: 2.507524013519287\n",
            "4900 loss: 2.5074851512908936\n",
            "5000 loss: 2.507446527481079\n",
            "5100 loss: 2.5074081420898438\n",
            "5200 loss: 2.5073697566986084\n",
            "5300 loss: 2.507331609725952\n",
            "5400 loss: 2.507293939590454\n",
            "5500 loss: 2.507256269454956\n",
            "5600 loss: 2.507218837738037\n",
            "5700 loss: 2.5071821212768555\n",
            "5800 loss: 2.5071451663970947\n",
            "5900 loss: 2.507108211517334\n",
            "6000 loss: 2.5070717334747314\n",
            "6100 loss: 2.507035732269287\n",
            "6200 loss: 2.5069997310638428\n",
            "6300 loss: 2.5069637298583984\n",
            "6400 loss: 2.5069282054901123\n",
            "6500 loss: 2.5068929195404053\n",
            "6600 loss: 2.5068578720092773\n",
            "6700 loss: 2.5068225860595703\n",
            "6800 loss: 2.5067882537841797\n",
            "6900 loss: 2.50675368309021\n",
            "7000 loss: 2.5067193508148193\n",
            "7100 loss: 2.5066850185394287\n",
            "7200 loss: 2.506650924682617\n",
            "7300 loss: 2.506617307662964\n",
            "7400 loss: 2.5065839290618896\n",
            "7500 loss: 2.5065505504608154\n",
            "7600 loss: 2.5065174102783203\n",
            "7700 loss: 2.5064845085144043\n",
            "7800 loss: 2.5064518451690674\n",
            "7900 loss: 2.5064194202423096\n",
            "8000 loss: 2.5063867568969727\n",
            "8100 loss: 2.506354570388794\n",
            "8200 loss: 2.5063223838806152\n",
            "8300 loss: 2.506290912628174\n",
            "8400 loss: 2.5062592029571533\n",
            "8500 loss: 2.506227731704712\n",
            "8600 loss: 2.5061964988708496\n",
            "8700 loss: 2.5061655044555664\n",
            "8800 loss: 2.5061347484588623\n",
            "8900 loss: 2.506103992462158\n",
            "9000 loss: 2.506073236465454\n",
            "9100 loss: 2.506042957305908\n",
            "9200 loss: 2.5060126781463623\n",
            "9300 loss: 2.5059826374053955\n",
            "9400 loss: 2.505952835083008\n",
            "9500 loss: 2.50592303276062\n",
            "9600 loss: 2.5058934688568115\n",
            "9700 loss: 2.505864143371582\n",
            "9800 loss: 2.5058350563049316\n",
            "9900 loss: 2.5058059692382812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 생성\n",
        "test_X = test_set.drop(\"WAR\", axis=1).to_numpy()\n",
        "test_X_scaled = std_scaler.fit_transform(X)\n",
        "test_y = test_set[\"WAR\"].copy().to_numpy()\n",
        "\n",
        "predictions = tf.matmul(test_X_scaled, W) + b"
      ],
      "metadata": {
        "id": "lZFfLs9tSMVD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOYc0B8KSr5f",
        "outputId": "bedf5b19-cb87-4d66-e073-9919ecbf1c11"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(40, 1), dtype=float32, numpy=\n",
              "array([[3.476756 ],\n",
              "       [3.4436867],\n",
              "       [3.5261447],\n",
              "       [3.2516813],\n",
              "       [3.2883716],\n",
              "       [3.43537  ],\n",
              "       [3.4230912],\n",
              "       [3.4889183],\n",
              "       [3.418394 ],\n",
              "       [3.3551924],\n",
              "       [3.3171792],\n",
              "       [3.2662306],\n",
              "       [3.214706 ],\n",
              "       [3.3763537],\n",
              "       [3.271538 ],\n",
              "       [3.4948442],\n",
              "       [3.3132873],\n",
              "       [3.406782 ],\n",
              "       [3.4752204],\n",
              "       [3.4863656],\n",
              "       [3.413218 ],\n",
              "       [3.4220204],\n",
              "       [3.4584348],\n",
              "       [3.421791 ],\n",
              "       [3.4539652],\n",
              "       [3.5185194],\n",
              "       [3.3439476],\n",
              "       [3.2163975],\n",
              "       [3.4582474],\n",
              "       [3.3697312],\n",
              "       [3.5402482],\n",
              "       [3.4632943],\n",
              "       [3.4355135],\n",
              "       [3.4240088],\n",
              "       [3.4066203],\n",
              "       [3.4287174],\n",
              "       [3.3442392],\n",
              "       [3.415227 ],\n",
              "       [3.3297904],\n",
              "       [3.4157238]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    }
  ]
}
